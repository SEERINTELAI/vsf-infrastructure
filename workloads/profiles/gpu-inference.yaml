# GPU Inference Workload Profile
# Simulates ML inference service (mock mode compatible)

name: gpu-inference-workload
type: deployment
namespace: vsf-workloads
replicas: 4
resources:
  requests:
    cpu: "250m"
    memory: "1Gi"
    nvidia.com/gpu: "1"
  limits:
    cpu: "1"
    memory: "2Gi"
    nvidia.com/gpu: "1"
intensity: 0.6
pattern: diurnal
duration_seconds: 900
image: nvcr.io/nvidia/cuda:12.0-base
command:
  - sh
  - -c
  - |
    echo "Starting GPU inference server"
    nvidia-smi || echo "Mock mode - no real GPU"
    # Simulate inference requests
    while true; do
      echo "Processing inference request"
      sleep 1
    done
node_selector:
  gpu: "true"
tolerations:
  - key: nvidia.com/gpu
    operator: Exists
    effect: NoSchedule
labels:
  workload-type: gpu-inference
  experiment: energy-optimization
  gpu-workload: "true"
  vsf-workload: "true"
